# Grasp Library Launch Options and Customization Notes
This tutorial documents the launch options which are used for customization. Each option will be introduced in the following format:
* **option_name** [**default_value**|other_values]: Description of this option. Customization Notes.

## GraspDetectorGPD Launch Options
* **cloud_topic** [**"/camera/depth_registered/points"**|"string"]: Name of point cloud topic as input to the grasp detection, default value compliant with an RGBD OpenNI camera.
* **device** [**0**|1|2|3]: Configure device for grasp pose inference to execute, 0 for CPU, 1 for GPU, 2 for VPU, 3 for FPGA. In case OpenVINO plug-ins are installed ([tutorial](install_openvino.md)), this configure deploy the CNN based deep learning inference on to the target device. Deploying the inference onto **GPU or VPU** will save CPU loads for other computation tasks.
* **auto_mode** [false|**true**]: Configure grasp detection mode. When auto_mode is true, Grasp Library works in sensor-driven mode, processing grasp detection when point cloud message arrives. When auto_mode is false, Grasp Library works in service-driven mode, processing grasp detection when a service request arrives. Configure to **service-driven** mode will save most CPU loads against that of the sensor-driven mode.
* **plane_remove** [**false**|true]: Configure whether or not remove the planes (like the table plane) from point cloud input. Enabling this helps to avoid generating grasp poses across the table.
* **workspace** [**[-1.0, 1.0, -1.0, 1.0, -1.0, 1,0]**|[1*6 double]]: Configure a boundry cube in camera frame for grasp generation and detection. *This need to be customized according to user's setup.*
* **finger_width** [**0.005**|double]: The finger thickness in metres. *This need to be customized according to user's robot hand.*
* **hand_outer_diameter** [**0.12**|double]: The maximum robot hand aperture in metres. *This need to be customized according to user's robot hand.*
* **hand_depth** [**0.06**|double]: The hand depth (the finger length) in metres. Tuning this parameter will affect the "GraspConfig::bottom" field (the hand base) in the grasp detection results. *This need to be customized according to user's robot hand.*
* **hand_height** [**0.02**|double]: The finger breadth in metres. *This need to be customized according to user's robot hand.*

## GraspPlanner Launch Options
* **grasp_service_timeout** [**5**|double]: Timeout in seconds for a service request waiting for grasp detection result. Grasp Planner will not take point could inputs from the history buffer. Indeed after receiving a service request, Grasp Planner will start grasp detection on the coming point cloud input. This parameter configures the timeout period for Grasp Planner to wait for the grasp detection result. Usually this's an amount of max latencies in RGBD sensor, Grasp Detector, Grasp Planner, any other nodes in the pipeline, additionally with an estimated worst delay in the system.
* **grasp_score_threshold** [**200**|integer]: Minimum score expected for grasps returned from this service.
* **grasp_frame_id** [**"base"**|"string"]: Frame id expected for grasps returned from this service. When this parameter is specified, Grasp Planner try to transform the grasp from the original frame (usually a camera's color frame) to this target frame, given the TF available.
* **grasp_approach** [**[0.0, 0.0, -1.0]**|[1*3 double]]: Specify expected approach direction in the target frame specified by 'grasp_frame_id'. Grasp Planner will return grasp poses with approach direction approximate to this parameter. This is useful when a MoveIt application wants to constraint the approaching direction. *This need to be customized according to user's setup.*
* **grasp_approach_angle** [**M_PI**|3.14|1.57|double]: Maximum angle in radian acceptable between the expected 'grasp_approach' and the real approach returned from this service. Default is [-M_PI, M_PI], which implies any approach directions are acceptable. *This need to be customized according to user's setup.*
* **grasp_offset** [**[0.0, 0.0, 0.0]**|[1*3 double]]: Offset [x, y, z] in metres applied to the grasps detected. This offset allows adjustment over the final grasp position, to overcome erros that might be accumulated from camera calibration, hand-eye calibration, grasp pose detection, etc. *This need to be customized according to user's setup.*
* **grasp_boundry** [**[-1.0, 1.0, -1.0, 1.0, -1.0, 1,0]**|[1*6 double]]: Boundry cube in grasp_frame_id expected for grasps returned from this service. This parameter takes effect only after transformation into the target frame specified by "grasp_frame_id". When the transformation is unavailalbe, boundry checking will be skipped, and in such case the "GraspDetectorGPD::workspace" parameter still takes effect. *This need to be customized according to user's setup.*
* **eef_offset** [**0.16**|double]: Offset in metres from the gripper base (finger root) to the parent link of gripper. The parent link is usually the end of the robot arm. *This need to be customized according to the gripper geometry.*
* **eef_yaw_offset** [**0.0**|double]: Gripper yaw offset to its parent link, in radian. *This need to be customized if the gripper has yaw offset to TCP (tool center point) of the robot arm.*
* **finger_joint_names** [**["panda_finger_joint1", "panda_finger_joint2"]**|[1*2 "string"]]: Joint names of gripper fingers. Joint names are filled into MoveIt's grasp interface, to control the posture of hand for the position of 'pre_grasp_posture' and 'grasp_posture' (see [moveit_msgs::msg::Grasp](http://docs.ros.org/api/moveit_msgs/html/msg/Grasp.html)). Joint names are usually defined in URDF of the robot hand. *This need to be customized according to user's setup.*

Other ROS parameters not mentioned here, refer to the codes [ros_params.cpp](../grasp_ros2/src/ros_params.cpp) for details.

## Customization Notes
* **Model training for grasp detection**: It depends on which back-end grasp detection algorithm is used. For [Grasp Pose Detection](https://github.com/atenpas/gpd), the model was trained with 185K labeled grasps and 55 object models from [bigBIRD](http://rll.berkeley.edu/bigbird). In case of any necessity to re-train, please refer to the discussion [#49](https://github.com/atenpas/gpd/issues/49) in the upstream project.
